{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BipedalWalker-v2 with stable_baselines library \n",
    "The aim of this super quick tutorial is to give a brief insight into stable_baselines library that offers many RL models ready to use within gym environments. It's very useful for model comparison. \n",
    "\n",
    "We need to find optimal policy for BipedalWalker-v2, which requires models capable of handling:\n",
    "* continous state space, \n",
    "* continous action space.\n",
    "\n",
    "One of such models is PPO2 (kind of Proximal Policy Optimization), which will be used for this example. Because stable_baselines gives us PPO2 implementation as a black-box, we can only play with hiperparameters to choose the best combination. A cell below trains 32 different agents and saves mean rewards from 100 test episodes to result table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 28.499265432357788s\n",
      "test time: 7.15754508972168s\n",
      "-113.71032981138535\n",
      "train time: 34.18337059020996s\n",
      "test time: 79.4946768283844s\n",
      "-85.901246414206\n",
      "train time: 22.919082403182983s\n",
      "test time: 29.26411747932434s\n",
      "-99.96409903066568\n",
      "train time: 26.913942575454712s\n",
      "test time: 42.41841101646423s\n",
      "-106.04775242702682\n",
      "train time: 27.160589933395386s\n",
      "test time: 74.47736740112305s\n",
      "-99.01373604460521\n",
      "train time: 34.53701186180115s\n",
      "test time: 7.246541738510132s\n",
      "-106.27053628453444\n",
      "train time: 23.31827998161316s\n",
      "test time: 67.67654037475586s\n",
      "-94.01656457732297\n",
      "train time: 26.832749605178833s\n",
      "test time: 21.82031273841858s\n",
      "-115.6099921328768\n",
      "train time: 26.974185466766357s\n",
      "test time: 99.27304792404175s\n",
      "-134.76463289532037\n",
      "train time: 32.98280644416809s\n",
      "test time: 4.908333778381348s\n",
      "-104.90583229940498\n",
      "train time: 22.78860330581665s\n",
      "test time: 61.065229654312134s\n",
      "-118.11756724076089\n",
      "train time: 25.998509645462036s\n",
      "test time: 92.72901725769043s\n",
      "-101.6604349305459\n",
      "train time: 26.26438021659851s\n",
      "test time: 5.5715343952178955s\n",
      "-114.52972256375143\n",
      "train time: 33.04588747024536s\n",
      "test time: 12.105873107910156s\n",
      "-108.88181368510294\n",
      "train time: 22.596642017364502s\n",
      "test time: 6.13254451751709s\n",
      "-116.30221782279354\n",
      "train time: 26.142985105514526s\n",
      "test time: 7.415485382080078s\n",
      "-109.19416843208222\n",
      "train time: 27.314517974853516s\n",
      "test time: 8.509437322616577s\n",
      "-108.59836142034244\n",
      "train time: 35.14509344100952s\n",
      "test time: 62.42444324493408s\n",
      "-88.48969606901593\n",
      "train time: 22.97602105140686s\n",
      "test time: 87.57775402069092s\n",
      "-91.92117481675716\n",
      "train time: 26.734490871429443s\n",
      "test time: 48.07228875160217s\n",
      "-102.87620123075347\n",
      "train time: 27.498191833496094s\n",
      "test time: 14.813937664031982s\n",
      "-110.40462018933553\n",
      "train time: 33.370511531829834s\n",
      "test time: 10.067863941192627s\n",
      "-115.25291097926998\n",
      "train time: 22.402255296707153s\n",
      "test time: 7.903566360473633s\n",
      "-105.50124219415528\n",
      "train time: 25.72299027442932s\n",
      "test time: 32.23520350456238s\n",
      "-107.26782588998547\n",
      "train time: 26.115209341049194s\n",
      "test time: 17.69284701347351s\n",
      "-112.40107741179996\n",
      "train time: 32.85559940338135s\n",
      "test time: 44.201234340667725s\n",
      "-118.60272959762109\n",
      "train time: 22.82347583770752s\n",
      "test time: 41.36674666404724s\n",
      "-115.48558167765718\n",
      "train time: 26.224834203720093s\n",
      "test time: 6.031216144561768s\n",
      "-113.73026779120613\n",
      "train time: 26.60792565345764s\n",
      "test time: 116.8687539100647s\n",
      "-136.55050222418902\n",
      "train time: 32.715033531188965s\n",
      "test time: 104.86660432815552s\n",
      "-131.87933617786976\n",
      "train time: 22.473402738571167s\n",
      "test time: 20.293434619903564s\n",
      "-118.53270151088715\n",
      "train time: 25.58178448677063s\n",
      "test time: 38.45606446266174s\n",
      "-108.60959820118705\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "\n",
    "import gym\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "from stable_baselines import PPO2\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make('BipedalWalker-v2')\n",
    "env = DummyVecEnv([lambda: env])  # The algorithms require a vectorized environment to run\n",
    "\n",
    "\n",
    "params = {\n",
    "    \"gammas\": [0.99, 0.999],\n",
    "    \"n_steps\":  [64, 128],\n",
    "    \"learning_rate\" :  [0.00025, 0.002],\n",
    "    \"max_grad_norm\" : [0.5, 1],\n",
    "    \"nminibatches\":  [4, 8]\n",
    "}\n",
    "# Define the model\n",
    "\n",
    "param_grid = ParameterGrid(params)\n",
    "\n",
    "df = pd.DataFrame({'gammas':[], \n",
    "                   'n_steps': [], \n",
    "                   'learning_rate': [], \n",
    "                   'max_grad_norm': [], \n",
    "                   'nminibatches': [], \n",
    "                   'rewards': []})\n",
    "\n",
    "for param_id in range(len(param_grid)):\n",
    "    gamma = param_grid[param_id][\"gammas\"]\n",
    "    n_steps = param_grid[param_id][\"n_steps\"]\n",
    "    learning_rate = param_grid[param_id][\"learning_rate\"]\n",
    "    max_grad_norm = param_grid[param_id][\"max_grad_norm\"]\n",
    "    nminibatches = param_grid[param_id][\"nminibatches\"]\n",
    "    \n",
    "    \n",
    "\n",
    "    model = PPO2(MlpPolicy, \n",
    "                 env, \n",
    "                 verbose=0, \n",
    "                 gamma=gamma, \n",
    "                 n_steps=n_steps, \n",
    "                 ent_coef=0.01, \n",
    "                 learning_rate=learning_rate, \n",
    "                 vf_coef=0.5, \n",
    "                 max_grad_norm=max_grad_norm, \n",
    "                 lam=0.95, \n",
    "                 nminibatches=nminibatches, \n",
    "                 noptepochs=4, \n",
    "                 cliprange=0.2)\n",
    "\n",
    "    n_epochs = 25000\n",
    "    train_start = time.time()\n",
    "    model.learn(total_timesteps=n_epochs)\n",
    "    print('train time: {}s'.format(time.time() - train_start))\n",
    "\n",
    "    test_start = time.time()\n",
    "    reward_history = []\n",
    "    for episode in range(100):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        rewards_in_episode = 0\n",
    "        while not done:\n",
    "            action, _states = model.predict(obs)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            rewards_in_episode += reward[0]\n",
    "        reward_history.append(rewards_in_episode)\n",
    "    mean_reward = sum(reward_history)/len(reward_history)\n",
    "    \n",
    "    _params = {\n",
    "    \"gammas\": [gamma],\n",
    "    \"n_steps\": [n_steps],\n",
    "    \"learning_rate\": [learning_rate],\n",
    "    \"max_grad_norm\": [max_grad_norm],\n",
    "    \"nminibatches\": [nminibatches],\n",
    "    \"rewards\": [mean_reward]\n",
    "    }\n",
    "    \n",
    "    df = df.append(pd.DataFrame.from_dict(_params))\n",
    "    print('test time: {}s'.format(time.time() - test_start))\n",
    "    print(mean_reward)\n",
    "    \n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the best hiperparameter set based on maximum reward criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_setup = df[df.rewards == df.rewards.max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>gammas</th>\n",
       "      <th>n_steps</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_grad_norm</th>\n",
       "      <th>nminibatches</th>\n",
       "      <th>rewards</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.99</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.00025</td>\n",
       "      <td>0.5</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-85.901246</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  gammas  n_steps  learning_rate  max_grad_norm  nminibatches  \\\n",
       "1      0    0.99     64.0        0.00025            0.5           8.0   \n",
       "\n",
       "     rewards  \n",
       "1 -85.901246  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Agent with the best hiperparameters set\n",
    "This time we will set number of training epochs to 500000 to obtain finest result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 648.748372554779s\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('BipedalWalker-v2')\n",
    "env = DummyVecEnv([lambda: env])  # The algorithms require a vectorized environment to run\n",
    "\n",
    "model = PPO2(MlpPolicy, \n",
    "                 env, \n",
    "                 verbose=0, \n",
    "                 gamma=best_setup.gammas.values[0], \n",
    "                 n_steps=int(best_setup.n_steps.values[0]), \n",
    "                 ent_coef=0.01, \n",
    "                 learning_rate=best_setup.learning_rate.values[0], \n",
    "                 vf_coef=0.5, \n",
    "                 max_grad_norm=best_setup.max_grad_norm.values[0], \n",
    "                 lam=0.95, \n",
    "                 nminibatches=int(best_setup.nminibatches.values[0]), \n",
    "                 noptepochs=4, \n",
    "                 cliprange=0.2)\n",
    "\n",
    "n_epochs = 500000\n",
    "train_start = time.time()\n",
    "model.learn(total_timesteps=n_epochs)\n",
    "print('train time: {}s'.format(time.time() - train_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see how it walks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "for i in range(1000):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record rendered trials and save it to MP4 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of rewards in one episode: -103.02607073885156\n",
      "Sum of rewards in one episode: -123.62441200384637\n",
      "Sum of rewards in one episode: -81.0249242716236\n",
      "Sum of rewards in one episode: -92.34792243220727\n",
      "Sum of rewards in one episode: -113.7854693334084\n",
      "Sum of rewards in one episode: -102.65251336753136\n",
      "Sum of rewards in one episode: -87.37850658874959\n",
      "Saving video to  /home/waldemar/Pictures/random-agent-BipedalWalker-v2-step-0-to-step-1003.mp4\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines.common.vec_env import VecVideoRecorder\n",
    "\n",
    "env_id = 'BipedalWalker-v2'\n",
    "video_folder = 'Pictures/'\n",
    "video_length = 1003\n",
    "env = DummyVecEnv([lambda: gym.make(env_id)])\n",
    "env = VecVideoRecorder(env, video_folder,\n",
    "                      record_video_trigger=lambda x: x == 0, video_length=video_length,\n",
    "                      name_prefix=\"random-agent-{}\".format(env_id))\n",
    "\n",
    "obs = env.reset()\n",
    "rewards_in_episode = 0\n",
    "for i in range(video_length+1):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, reward, done, info = env.step(action)     \n",
    "    rewards_in_episode += reward[0]\n",
    "    \n",
    "    if done:\n",
    "        print('Sum of rewards in one episode: {}'.format(rewards_in_episode))\n",
    "        rewards_in_episode = 0\n",
    "        \n",
    "    env.render()\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
